{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    ########### TRAINING ##############\\n    clf = svm.SVC(kernel=\\'rbf\\',decision_function_shape=\\'ovo\\' ,class_weight=\\'balanced\\'  ,max_iter=-1)\\n    clf.fit(x_train, y_train)\\n\\n    #**** predict label for Test data\\n    x_test=scaler.transform(x_test)\\n    x_test=pca.transform(x_test)\\n    label_predict_test=clf.predict(x_test)\\n\\n    #*****\\n    cm = ConfusionMatrix(actual_vector=y_test, predict_vector=label_predict_test)\\n    CM=np.array([list(row.values()) for row in list(cm.matrix.values())])\\n    plot_CM(CM,labels_name,\\'CM_ILD_Test\\'+name,True)\\n\\n    #*****\\n    accuracy=np.array(list((cm.ACC).values()))\\n    precision=np.array(list((cm.PPV).values()))\\n    recall=np.array(list((cm.TPR).values()))\\n    true_negative_rate=np.array(list((cm.TNR).values()))\\n    AUC=np.array(list((cm.AUC).values()))\\n    F1=np.array(list((cm.F1).values()))\\n    overall_accuracy=round(100*cm.Overall_ACC,2)\\n    overal_F1=round(100*cm.F1_Macro,2)\\n\\n    df=pd.DataFrame(\\n        {\"Accuracy\":np.round(100*accuracy,2),\\n        \"Recall\":np.round(100*recall,2),\\n        \"Precision\":np.round(100*precision,2),\\n        \"TN rate\":np.round(100*true_negative_rate,2),\\n        \"AUC\":np.round(100*AUC,2),\\n        \"F1\":np.round(100*F1,2)})\\n    df=df.rename(index=dict((v-1,k) for k,v in labels_dict.items()))\\n    with open(\\'result/ILD_Test\\'+name+\\'.txt\\', mode=\\'w\\') as file_object:\\n        print(df, file=file_object)\\n        print(\\'\\n\\t**** overal accuracy: \\'+str(overall_accuracy)+\\' % \\', file=file_object)\\n        print(\\'\\t**** overal F1-score (Macro): \\'+str(overal_F1)+\\' %\\', file=file_object)\\n    #*****\\n    print(\\'\\nFeature vector length\\'+name+\\' :\\',Train.shape[1]-1)\\n    print(\\'SVM, RBF kernel,PCA components: \\',pca_num)\\n    print(\\'\\tAccuracy on Test: \\' ,overall_accuracy,\\'%\\')\\n    print(\\'\\tF1_score on Test: \\',overal_F1,\\'%\\')\\n\\n\\n'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pycm import *\n",
    "import scipy.io\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try: os.mkdir(\"result\")\n",
    "except: pass\n",
    "\n",
    "labels_name=['healthy', 'ground', 'micronodules', 'emphysema', 'fibrosis']\n",
    "labels_dict={'healthy': 1, 'ground': 2, 'micronodules': 3, 'emphysema': 4, 'fibrosis': 5}\n",
    "\n",
    "\n",
    "def plot_CM(CM,labels_name,name,is_save):\n",
    "    fig=plt.figure(figsize=(12, 12))\n",
    "    plt.imshow(CM, cmap=plt.cm.Blues);\n",
    "    for i in range(CM.shape[0]):\n",
    "        for j in range(CM.shape[0]):\n",
    "            if(CM[i, j] > CM.max()/2) : color=\"white\"\n",
    "            else: color=\"black\"\n",
    "            plt.text(j, i,CM[i, j] ,horizontalalignment=\"center\",color=color,fontsize=17)\n",
    "    plt.xticks(np.arange(CM.shape[0]), labels_name,fontsize='x-large',rotation=-30,fontweight='bold')\n",
    "    plt.yticks(np.arange(CM.shape[0]),  labels_name,fontsize='x-large',fontweight='bold')\n",
    "    plt.title(name,fontsize=18,fontweight='bold'); plt.ylabel('True label',fontsize=18); plt.xlabel('Predicted label',fontsize=18);\n",
    "    if is_save:\n",
    "        fig.savefig(\"result/\"+name+'.jpg', dpi=3*fig.dpi)\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### read features and concert to dataframe\n",
    "feature_name='texture_features'\n",
    "label_ILD=[]\n",
    "features=[]\n",
    "for ll in labels_name:\n",
    "    label_path=os.path.join(feature_name+'/',ll)\n",
    "    files=os.listdir(label_path)\n",
    "    try: files.remove('.DS_Store')\n",
    "    except: pass\n",
    "    for row in files:\n",
    "        file_path=os.path.join(label_path,row)\n",
    "        feature_vector = scipy.io.loadmat(file_path)['feature_vector']\n",
    "\n",
    "        # features.append(np.concatenate([feature_vector[0][:3*258],feature_vector[0][258*4:1080+258],feature_vector[0][1080+258*2: 1080+3*258],feature_vector[0][1080+258*4:]]))\n",
    "        features.append(feature_vector[0])\n",
    "    label_ILD+=[labels_dict[ll]]*len(files)\n",
    "features=np.array(features)\n",
    "label_ILD=np.array(label_ILD)\n",
    "label_ILD = np.reshape(label_ILD,(features.shape[0],1))\n",
    "\n",
    "\n",
    "for same_class_size in [False ]:\n",
    "    if same_class_size: name=\"_same_class_size\"\n",
    "    else: name=\"\"\n",
    "\n",
    "    ########### select Train and Test sets #############\n",
    "    # split data between train and test \n",
    "    # we choose 25% of data for Test\n",
    "    # after selecting Train , test we shuffles each set using unison_shuffled_copies\n",
    "    ####################################################\n",
    "    Data=pd.DataFrame(np.concatenate([features, label_ILD],axis=1))\n",
    "    if same_class_size:\n",
    "        class_size=np.min(Data[Data.shape[1]-1].value_counts())\n",
    "        Data=Data.groupby(Data.shape[1]-1).apply(lambda s: s.sample(n=class_size,replace=False,random_state=0))\n",
    "        Data = Data.reset_index(level=[None])\n",
    "        Data=Data.set_index('level_1')\n",
    "\n",
    "    Data=Data.sample(frac=1,random_state=5) ## shuffle\n",
    "    Train=Data.sample(frac=0.75,replace=False,random_state=0)\n",
    "    Test= Data.drop(index=Train.index)\n",
    "\n",
    "    x_train =  Train.loc[:,[i for i in range(Train.shape[1]-1)]].values\n",
    "    y_train = Train.loc[:,[Train.shape[1]-1]].values.ravel()\n",
    "\n",
    "    x_test =  Test.loc[:,[i for i in range(Test.shape[1]-1)]].values\n",
    "    y_test = Test.loc[:,[Test.shape[1]-1]].values.ravel()\n",
    "\n",
    "    ########### Data preprocessing ##############\n",
    "    #  Data preprocessing is:\n",
    "    #       1) zero-mean and scale variances to one \n",
    "    #       2) PCA for 0.95% of total varince\n",
    "    #############################################\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    pca.fit(x_train)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    pca_num = np.argmax(cumsum > 0.9999999)\n",
    "    pca = PCA(n_components=pca_num)\n",
    "    x_train = pca.fit_transform(x_train)\n",
    "\n",
    "\n",
    "    ########### TRAINING ##############\n",
    "    clf = svm.SVC(kernel='rbf',decision_function_shape='ovo' ,class_weight='balanced'  ,max_iter=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    #**** predict label for Test data\n",
    "    x_test=scaler.transform(x_test)\n",
    "    x_test=pca.transform(x_test)\n",
    "    label_predict_test=clf.predict(x_test)\n",
    "\n",
    "    #*****\n",
    "    cm = ConfusionMatrix(actual_vector=y_test, predict_vector=label_predict_test)\n",
    "    CM=np.array([list(row.values()) for row in list(cm.matrix.values())])\n",
    "    plot_CM(CM,labels_name,'CM_ILD_Test'+name,True)\n",
    "\n",
    "    #*****\n",
    "    accuracy=np.array(list((cm.ACC).values()))\n",
    "    precision=np.array(list((cm.PPV).values()))\n",
    "    recall=np.array(list((cm.TPR).values()))\n",
    "    true_negative_rate=np.array(list((cm.TNR).values()))\n",
    "    AUC=np.array(list((cm.AUC).values()))\n",
    "    F1=np.array(list((cm.F1).values()))\n",
    "    overall_accuracy=round(100*cm.Overall_ACC,2)\n",
    "    overal_F1=round(100*cm.F1_Macro,2)\n",
    "\n",
    "    df=pd.DataFrame(\n",
    "        {\"Accuracy\":np.round(100*accuracy,2),\n",
    "        \"Recall\":np.round(100*recall,2),\n",
    "        \"Precision\":np.round(100*precision,2),\n",
    "        \"TN rate\":np.round(100*true_negative_rate,2),\n",
    "        \"AUC\":np.round(100*AUC,2),\n",
    "        \"F1\":np.round(100*F1,2)})\n",
    "    df=df.rename(index=dict((v-1,k) for k,v in labels_dict.items()))\n",
    "    with open('result/ILD_Test'+name+'.txt', mode='w') as file_object:\n",
    "        print(df, file=file_object)\n",
    "        print('\\n\\t**** overal accuracy: '+str(overall_accuracy)+' % ', file=file_object)\n",
    "        print('\\t**** overal F1-score (Macro): '+str(overal_F1)+' %', file=file_object)\n",
    "    #*****\n",
    "    print('\\nFeature vector length'+name+' :',Train.shape[1]-1)\n",
    "    print('SVM, RBF kernel,PCA components: ',pca_num)\n",
    "    print('\\tAccuracy on Test: ' ,overall_accuracy,'%')\n",
    "    print('\\tF1_score on Test: ',overal_F1,'%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I2_hvg_lattice': [29.005, {'Pk': 0, 'Z': 29.005}],\n",
       " 'I2_ivg_lattice': [23.958, {'Pk': 0, 'Z': 23.958}],\n",
       " 'I_ivg_lattice': [17.588, {'Pk': 0.498, 'Z': 17.091}],\n",
       " 'I_hvg_lattice': [10.478, {'Pk': 0.676, 'Z': 9.801}],\n",
       " 'I_hvg_Nolattice': [7.175, {'Pk': 0.005, 'Z': 7.17}],\n",
       " 'I2_wavelet': [6.26],\n",
       " 'I_wavelet': [4.634],\n",
       " 'I_ivg_Nolattice': [0.9, {'Pk': 0.326, 'Z': 0.574}],\n",
       " 'I2_hvg_Nolattice': [0.002, {'Pk': 0, 'Z': 0.002}],\n",
       " 'I2_ivg_Nolattice': [0.0, {'Pk': 0, 'Z': 0.0}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res={\n",
    "    'I_hvg_lattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I_hvg_Nolattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I_ivg_lattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I_ivg_Nolattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I_wavelet':[0],\n",
    "    'I2_hvg_lattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I2_hvg_Nolattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I2_ivg_lattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I2_ivg_Nolattice':[0,{'Pk':0,'Z':0}],\n",
    "    'I2_wavelet':[0]\n",
    "    }\n",
    "\n",
    "n_pcs= pca.components_.shape[0]\n",
    "most_important = [np.abs(pca.components_[i]).argmax()+1 for i in range(n_pcs)]\n",
    "for i,row in enumerate(most_important):\n",
    "    \n",
    "    if 1<=row<=258 : \n",
    "        res['I_hvg_lattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I_hvg_lattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I_hvg_lattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "    \n",
    "    elif 258+1<=row<=258+258 :\n",
    "        res['I_hvg_Nolattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I_hvg_Nolattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I_hvg_Nolattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    elif 2*258+1<=row<=258+258*2 :\n",
    "        res['I_ivg_lattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I_ivg_lattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I_ivg_lattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    elif 3*258+1<=row<=258+258*3 :\n",
    "        res['I_ivg_Nolattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I_ivg_Nolattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I_ivg_Nolattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    if 1080+1<=row<=258+1080 : \n",
    "        res['I2_hvg_lattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I2_hvg_lattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I2_hvg_lattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    elif 1080+258+1<=row<=258+258+1080 :\n",
    "        res['I2_hvg_Nolattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I2_hvg_Nolattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I2_hvg_Nolattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    elif 1080+2*258+1<=row<=258+258*2+1080 :\n",
    "        res['I2_ivg_lattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I2_ivg_lattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I2_ivg_lattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "    elif 1080+3*258+1<=row<=258+258*3+1080 :\n",
    "        res['I2_ivg_Nolattice'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "        if row%258 in [1,2]:\n",
    "            res['I2_ivg_Nolattice'][1]['Pk']+=pca.explained_variance_ratio_[i]*100\n",
    "        else:\n",
    "            res['I2_ivg_Nolattice'][1]['Z']+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "\n",
    "    elif 4*258+1<=row<=4*258+48 :\n",
    "        res['I_wavelet'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "    elif 1080+4*258+1<=row<=4*258+48+1080 :\n",
    "        res['I2_wavelet'][0]+=pca.explained_variance_ratio_[i]*100\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "for key in res.keys():\n",
    "    res[key][0]= round(res[key][0],3)\n",
    "    try:\n",
    "        res[key][1]['Z']= round(res[key][1]['Z'],3)\n",
    "        res[key][1]['Pk']= round(res[key][1]['Pk'],3)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "res={k: v for k, v in sorted(res.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "******\n",
    "******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    11629\n",
       "1.0     7828\n",
       "5.0     5251\n",
       "2.0     2986\n",
       "4.0     1981\n",
       "Name: 1401, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels_name=['healthy', 'ground', 'micronodules', 'emphysema', 'fibrosis']\n",
    "labels_dict={'healthy': 1, 'ground': 2, 'micronodules': 3, 'emphysema': 4, 'fibrosis': 5}\n",
    "\n",
    "\n",
    "### read features and concert to dataframe\n",
    "feature_name='texture_features/graph'\n",
    "label_ILD=[]\n",
    "features_graph=[]\n",
    "for ll in labels_name:\n",
    "    label_path=os.path.join(feature_name+'/',ll)\n",
    "    files=os.listdir(label_path)\n",
    "    try: files.remove('.DS_Store')\n",
    "    except: pass\n",
    "    for row in files:\n",
    "        file_path=os.path.join(label_path,row)\n",
    "        feature_vector = scipy.io.loadmat(file_path)['feature_vector'] \n",
    "        features_graph.append(feature_vector[0])\n",
    "    label_ILD+=[labels_dict[ll]]*len(files)\n",
    "features_graph=np.array(features_graph)\n",
    "label_ILD=np.array(label_ILD)\n",
    "label_ILD = np.reshape(label_ILD,(features_graph.shape[0],1))\n",
    "\n",
    "feature_name='texture_features/wavelet'\n",
    "features_wavelet=[]\n",
    "for ll in labels_name:\n",
    "    label_path=os.path.join(feature_name+'/',ll)\n",
    "    files=os.listdir(label_path)\n",
    "    try: files.remove('.DS_Store')\n",
    "    except: pass\n",
    "    for row in files:\n",
    "        file_path=os.path.join(label_path,row)\n",
    "        feature_vector = scipy.io.loadmat(file_path)['feature_vector']\n",
    "        features_wavelet.append(feature_vector[0])\n",
    "features_wavelet=np.array(features_wavelet)\n",
    "\n",
    "\n",
    "\n",
    "Data=pd.DataFrame(np.concatenate([features_graph, features_wavelet, label_ILD],axis=1))\n",
    "Data[Data.shape[1]-1].value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature vector length : 1401\n",
      "SVM, RBF kernel,PCA components:  583\n",
      "\tAccuracy on Test:  95.61 %\n",
      "\tF1_score on Test:  95.56 %\n",
      "\n",
      "Feature vector length_same_class_size : 1401\n",
      "SVM, RBF kernel,PCA components:  567\n",
      "\tAccuracy on Test:  91.72 %\n",
      "\tF1_score on Test:  91.75 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for same_class_size in [False ,True ]:\n",
    "    if same_class_size: name=\"_same_class_size\"\n",
    "    else: name=\"\"\n",
    "\n",
    "    ########### select Train and Test sets #############\n",
    "    # split data between train and test \n",
    "    # we choose 25% of data for Test\n",
    "    # after selecting Train , test we shuffles each set using unison_shuffled_copies\n",
    "    ####################################################\n",
    "    Data=pd.DataFrame(np.concatenate([features_graph, features_wavelet, label_ILD],axis=1))\n",
    "    if same_class_size:\n",
    "        class_size=np.min(Data[Data.shape[1]-1].value_counts())\n",
    "        Data=Data.groupby(Data.shape[1]-1).apply(lambda s: s.sample(n=class_size,replace=False,random_state=0))\n",
    "        Data = Data.reset_index(level=[None])\n",
    "        Data=Data.set_index('level_1')\n",
    "\n",
    "    Data=Data.sample(frac=1,random_state=5) ## shuffle\n",
    "    Train=Data.sample(frac=0.75,replace=False,random_state=0)\n",
    "    Test= Data.drop(index=Train.index)\n",
    "\n",
    "    x_train =  Train.loc[:,[i for i in range(Train.shape[1]-1)]].values\n",
    "    y_train = Train.loc[:,[Train.shape[1]-1]].values.ravel()\n",
    "\n",
    "    x_test =  Test.loc[:,[i for i in range(Test.shape[1]-1)]].values\n",
    "    y_test = Test.loc[:,[Test.shape[1]-1]].values.ravel()\n",
    "\n",
    "    ########### Data preprocessing ##############\n",
    "    #  Data preprocessing is:\n",
    "    #       1) zero-mean and scale variances to one \n",
    "    #       2) PCA for 0.95% of total varince\n",
    "    #############################################\n",
    "    scaler = StandardScaler()\n",
    "    pca = PCA()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    pca.fit(x_train)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    pca_num = np.argmax(cumsum > 0.95)\n",
    "    pca = PCA(n_components=pca_num)\n",
    "    x_train = pca.fit_transform(x_train)\n",
    "\n",
    "\n",
    "    ########### TRAINING ##############\n",
    "    clf = svm.SVC(kernel='rbf',decision_function_shape='ovo' ,class_weight='balanced'  ,max_iter=-1)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    #**** predict label for Test data\n",
    "    x_test=scaler.transform(x_test)\n",
    "    x_test=pca.transform(x_test)\n",
    "    label_predict_test=clf.predict(x_test)\n",
    "\n",
    "    #*****\n",
    "    cm = ConfusionMatrix(actual_vector=y_test, predict_vector=label_predict_test)\n",
    "\n",
    "    #*****\n",
    "    accuracy=np.array(list((cm.ACC).values()))\n",
    "    precision=np.array(list((cm.PPV).values()))\n",
    "    recall=np.array(list((cm.TPR).values()))\n",
    "    true_negative_rate=np.array(list((cm.TNR).values()))\n",
    "    AUC=np.array(list((cm.AUC).values()))\n",
    "    F1=np.array(list((cm.F1).values()))\n",
    "    overall_accuracy=round(100*cm.Overall_ACC,2)\n",
    "    overal_F1=round(100*cm.F1_Macro,2)\n",
    "\n",
    "    df=pd.DataFrame(\n",
    "        {\"Accuracy\":np.round(100*accuracy,2),\n",
    "        \"Recall\":np.round(100*recall,2),\n",
    "        \"Precision\":np.round(100*precision,2),\n",
    "        \"TN rate\":np.round(100*true_negative_rate,2),\n",
    "        \"AUC\":np.round(100*AUC,2),\n",
    "        \"F1\":np.round(100*F1,2)})\n",
    "    df=df.rename(index=dict((v-1,k) for k,v in labels_dict.items()))\n",
    "\n",
    "    print('\\nFeature vector length'+name+' :',Train.shape[1]-1)\n",
    "    print('SVM, RBF kernel,PCA components: ',pca_num)\n",
    "    print('\\tAccuracy on Test: ' ,overall_accuracy,'%')\n",
    "    print('\\tF1_score on Test: ',overal_F1,'%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "857970f990130bbcaee778cf1846f7875676d945310dca1379fe4b5ef3d258a5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
